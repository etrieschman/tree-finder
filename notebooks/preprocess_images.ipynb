{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "594e7af7-e9e5-4c7c-8f92-6e3a29132535",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# constants\n",
    "repo_path = '/Users/etriesch/dev/tree-finder/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58b7eb54-417f-4045-a970-31fa965a0698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "dtype = torch.float32 # We will be using float throughout this tutorial.\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss.\n",
    "print_every = 100\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6b2589-9c62-421f-b2e2-eeeb6d76cccd",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8972c26e-534c-420b-9557-d16c7d6b15c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    N = x.shape[0] # read in N, C, H, W\n",
    "    return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return flatten(x)\n",
    "\n",
    "def check_accuracy(loader, model): \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "\n",
    "def train_model(model, optimizer, epochs=1):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "                check_accuracy(loader_val, model)\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f46233d-6c83-4230-998e-02a19e599223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to download public dataset\n",
    "# mnist_dataset = torchvision.datasets.MNIST(root=\"./data\",   download=True, train=True, transform=transforms.ToTensor())\n",
    "\n",
    "H, W = 32, 32\n",
    "\n",
    "# define transformations\n",
    "transform = T.Compose([T.Resize(H), T.CenterCrop(H), T.ToTensor()])\n",
    "treeds = dset.ImageFolder(repo_path + 'data/images', transform=transform)\n",
    "# TODO -- visualize what transofrm is doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92a3499f-31ee-4c3e-88e7-6cf6ee7aa186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(train, val, test): 1316 246 84\n"
     ]
    }
   ],
   "source": [
    "# make dataloaders\n",
    "PCT_TRAIN, PCT_VAL = 0.80, 0.15\n",
    "N = len(treeds)\n",
    "num_train = int(N * PCT_TRAIN)\n",
    "num_val = int(N * PCT_VAL)\n",
    "num_test = N - num_train - num_val\n",
    "print('(train, val, test):', num_train, num_val, num_test)\n",
    "\n",
    "\n",
    "loader_train = DataLoader(treeds, batch_size=64,\n",
    "                           sampler=sampler.SubsetRandomSampler(range(num_train)))\n",
    "loader_val = DataLoader(treeds, batch_size=64,\n",
    "                           sampler=sampler.SubsetRandomSampler(range(num_train, num_train+num_val)))\n",
    "loader_test = DataLoader(treeds, batch_size=64,\n",
    "                           sampler=sampler.SubsetRandomSampler(range(num_train+num_val, N)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baaf993-0898-4c7e-973c-1d1386c9f5c9",
   "metadata": {},
   "source": [
    "### Fully connected single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0b34e54-c600-4dcb-b599-60bf45fa35f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 2.2941\n",
      "Got 0 / 246 correct (0.00)\n",
      "\n",
      "Iteration 0, loss = 1.6417\n",
      "Got 16 / 246 correct (6.50)\n",
      "\n",
      "Iteration 0, loss = 1.5412\n",
      "Got 6 / 246 correct (2.44)\n",
      "\n",
      "Iteration 0, loss = 1.3936\n",
      "Got 28 / 246 correct (11.38)\n",
      "\n",
      "Iteration 0, loss = 1.4525\n",
      "Got 27 / 246 correct (10.98)\n",
      "\n",
      "Iteration 0, loss = 1.4922\n",
      "Got 27 / 246 correct (10.98)\n",
      "\n",
      "Iteration 0, loss = 1.6318\n",
      "Got 42 / 246 correct (17.07)\n",
      "\n",
      "Iteration 0, loss = 1.4514\n",
      "Got 31 / 246 correct (12.60)\n",
      "\n",
      "Iteration 0, loss = 1.4184\n",
      "Got 31 / 246 correct (12.60)\n",
      "\n",
      "Iteration 0, loss = 1.1273\n",
      "Got 37 / 246 correct (15.04)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_size = 4000\n",
    "learning_rate = 1e-2\n",
    "\n",
    "model = nn.Sequential(\n",
    "    Flatten(),\n",
    "    nn.Linear(3 * H * W, hidden_layer_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_layer_size, 10),\n",
    ")\n",
    "\n",
    "# you can use Nesterov momentum in optim.SGD\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                     momentum=0.9, nesterov=True)\n",
    "\n",
    "train_model(model, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13485da-6eb6-4c71-946f-460501dad37d",
   "metadata": {},
   "source": [
    "### Simple 3-layer convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd95d81a-715e-4a60-a93f-a5d4027f86d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_1 = 32\n",
    "channel_2 = 16\n",
    "learning_rate = 1e-2\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=channel_1, kernel_size=(5,5), padding=2),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(in_channels=channel_1, out_channels=channel_2, kernel_size=(3,3), padding=1),\n",
    "    nn.ReLU(), \n",
    "    Flatten(),\n",
    "    nn.Linear(channel_2*H*W, num_classes),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tree-finder",
   "language": "python",
   "name": "tree-finder"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
